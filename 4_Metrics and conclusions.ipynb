{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "140d3a4c",
   "metadata": {},
   "source": [
    "# Metrics and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f357e428",
   "metadata": {},
   "source": [
    "In this last point we will discuss the results obtained for both models. At the end, we will see future implementations that could be done to improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb0042",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "The first model we used in notebook number 3 was a linear regression model. This is an easy method to implement and quickly executed. The results it produced were:\n",
    "<img src=\"LRmodel.png\">\n",
    "As we can see, the main diagonal of the confusion matrix presents the largest amount of data. Having an accuracity score of 0.83. In addition, our model has similar values when it is wrong, both in false negatives and in false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3518392a",
   "metadata": {},
   "source": [
    "### RNN + word embedding model\n",
    "This model was the most complex to implement. Additionally, it introduced numerous dependency bugs that took a long time to implement. At the time of implementation, each epoch took 20 minutes (using GPUs). The results it produced were:\n",
    "<img src=\"RNNmodel.png\">\n",
    "It is seen that an accuracity of 0.8594 has been obtained in three epochs. It would have been very interesting to see how the model is trained for a larger number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a163a6d6",
   "metadata": {},
   "source": [
    "#### We conclude that the best model is the RNN with word embedding layer. In addition, it could present greater potential if it could have worked with a greater vocabulary and increase the number of times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defdedcc",
   "metadata": {},
   "source": [
    "### Future implementations\n",
    "The notebook 3 preprocessing function did not make use of lemmatization or stemmatization. Perhaps this could have helped reduce the cardinality of the vocabulary and optimize the results.\n",
    "With respect to the RNN, the number of intermediate layers and neurons of the LSTM could have been varied to check how the results would vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf166af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
